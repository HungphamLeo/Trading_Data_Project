services:

  # -----------------------
  # Airflow Database Init (RUN ONCE)
  # -----------------------
  airflow-init:
    build:
      context: ..
      dockerfile: infra/docker/airflow/Dockerfile
    container_name: airflow-init
    environment:
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://postgres:password@postgres-local:5432/etl_project
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__FERNET_KEY: "YlCImzjge_TeZc5FVOrVCmOgvkV6jN-ESsZpmkwPe1g="
      PYTHONPATH: /opt/airflow:/opt/airflow/shared:/opt/airflow/platforms:/opt/airflow/scripts
    depends_on:
      postgres-local:
        condition: service_healthy
    command: >
      bash -c "
        airflow db init &&
        airflow users create \
          --username admin \
          --firstname Admin \
          --lastname User \
          --role Admin \
          --email admin@example.com \
          --password admin
      "
    networks:
      - etl-net
    restart: "no"

  # -----------------------
  # Airflow Webserver
  # -----------------------
  airflow-webserver:
    build:
      context: ..
      dockerfile: infra/docker/airflow/Dockerfile
    container_name: airflow-webserver
    environment:
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://postgres:password@postgres-local:5432/etl_project
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__FERNET_KEY: "YlCImzjge_TeZc5FVOrVCmOgvkV6jN-ESsZpmkwPe1g="
      AIRFLOW__WEBSERVER__SECRET_KEY: "your-secret-key-here"
      PYTHONPATH: /opt/airflow:/opt/airflow/shared:/opt/airflow/platforms:/opt/airflow/scripts
    volumes:
      - ../platforms/processing/airflow/dags:/opt/airflow/dags
      - ../shared:/opt/airflow/shared
      - ../platforms:/opt/airflow/platforms
      - ../scripts:/opt/airflow/scripts
    ports:
      - "8080:8080"
    command: webserver
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      mongo-local:
        condition: service_healthy
      postgres-local:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    networks:
      - etl-net
    restart: always

  # -----------------------
  # Airflow Scheduler
  # -----------------------
  airflow-scheduler:
    build:
      context: ..
      dockerfile: infra/docker/airflow/Dockerfile
    container_name: airflow-scheduler
    environment:
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://postgres:password@postgres-local:5432/etl_project
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__FERNET_KEY: "YlCImzjge_TeZc5FVOrVCmOgvkV6jN-ESsZpmkwPe1g="
      PYTHONPATH: /opt/airflow:/opt/airflow/shared:/opt/airflow/platforms:/opt/airflow/scripts
    volumes:
      - ../platforms/processing/airflow/dags:/opt/airflow/dags
      - ../shared:/opt/airflow/shared
      - ../platforms:/opt/airflow/platforms
      - ../scripts:/opt/airflow/scripts
    command: scheduler
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      mongo-local:
        condition: service_healthy
      postgres-local:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type SchedulerJob --hostname $(hostname)"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    networks:
      - etl-net
    restart: always

  # -----------------------
  # Prefect Server UI/API
  # -----------------------
  prefect-server:
    image: prefecthq/prefect:2-latest
    container_name: prefect-server
    ports:
      - "4200:4200"
    command: prefect server start --host 0.0.0.0
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:4200/api/health || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 10
      start_period: 60s
    networks:
      - etl-net
    restart: always

  # -----------------------
  # Prefect Worker
  # -----------------------
  prefect-worker:
    build:
      context: ..
      dockerfile: infra/docker/prefect/Dockerfile
    container_name: prefect-worker
    depends_on:
      prefect-server:
        condition: service_healthy
    environment:
      PREFECT_API_URL: http://prefect-server:4200/api
      PYTHONPATH: /app:/app/shared:/app/platforms:/app/scripts
    volumes:
      - ../platforms/processing/prefect/flows:/app/flows
      - ../shared:/app/shared
      - ../platforms:/app/platforms
      - ../scripts:/app/scripts
    command: ["/entrypoint.sh"]
    networks:
      - etl-net
    restart: always

  # -----------------------
  # MongoDB
  # -----------------------
  mongo-local:
    image: mongo:7.0
    container_name: mongo-local
    restart: always
    environment:
      MONGO_INITDB_ROOT_USERNAME: root
      MONGO_INITDB_ROOT_PASSWORD: root123
    ports:
      - "27017:27017"
    volumes:
      - mongo_data:/data/db
    healthcheck:
      test: ["CMD", "mongosh", "--quiet", "--eval", "db.adminCommand('ping').ok"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 40s
    networks:
      - etl-net

  # -----------------------
  # PostgreSQL
  # -----------------------
  postgres-local:
    image: postgres:15
    container_name: postgres-local
    restart: always
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: password
      POSTGRES_DB: etl_project
    ports:
      - "5432:5432"
    volumes:
      - pg_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - etl-net

  # -----------------------
  # Hadoop Namenode
  # -----------------------
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    environment:
      CLUSTER_NAME: hadoop
      CORE_CONF_fs_defaultFS: hdfs://namenode:9000
      HDFS_CONF_dfs_namenode_name_dir: file:///hadoop/dfs/name
    ports:
      - "9870:9870"
      - "9000:9000"
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9870 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 90s
    networks:
      - etl-net
    restart: always

  # -----------------------
  # Hadoop Datanode
  # -----------------------
  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    depends_on:
      namenode:
        condition: service_healthy
    environment:
      CLUSTER_NAME: hadoop
      SERVICE_PRECONDITION: "namenode:9870"
      CORE_CONF_fs_defaultFS: hdfs://namenode:9000
    ports:
      - "9864:9864"
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9864 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    networks:
      - etl-net
    restart: always

# =======================================================
# VOLUMES & NETWORK
# =======================================================

volumes:
  mongo_data:
  pg_data:
  hadoop_namenode:
  hadoop_datanode:

networks:
  etl-net:
    driver: bridge